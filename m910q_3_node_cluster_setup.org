:PROPERTIES:
:ID:       80f7989c-e69a-469e-8a93-7db7de7dff19
:END:
#+title: M910Q - 3 Node Cluster Setup
#+hugo_custom_front_matter: :summary 
#+hugo_date: [2023-09-28 Thu]
#+hugo_auto_set_lastmod: Time-stamp: <2023-11-14 08:33:07 (adelgado)>
#+hugo_code_fence: nil
#+hugo_section: synapse
#+hugo_draft: false
#+hugo_tags: 
#+property: header-args :dir run :mkdirp yes :padline no :exports both :results code :eval no-export :tangle no
#+export_file_name: m910q_3_node_cluster_setup

First we install Arch on all three machines as our base operating system.

** Setup Static IP Address via ~systemd~

On each machine, referred to as a ~node~ going forward, we will set a static IP.

Find network card name
#+begin_src shell
ip link
#+end_src

Create a network profile for the network card (it is recommended to prefix a number to the filename):
#+begin_src shell
sudo vim /etc/systemd/network/10-enp0s31f6.network
#+end_src

~Address~ and ~Gateway~ are examples here and should be values you define based on your subnet
#+begin_src bash
#10-enp0s31f6.network
[Match]
Name=enp0s31f6

[Network]
Address=192.168.1.102/24
Gateway=192.168.1.1
#+end_src

Enable ~systemd-networkd~ and reboot
#+begin_src bash
sudo systemctl enable --now systemd-networkd && sudo reboot
#+end_src

** Install kube-vip

~kube-vip~ is an open source HA and load-balancer for both control planes and services, and we'll utilize ~kube-vip~ for our baremetal in substitution of HAProxy and Keepalived.

Before beginning make sure the ~/etc/hosts~ on all your nodes is setup:

#+begin_src bash
127.0.0.1       localhost
::1             localhost
127.0.0.1       wunjo wunjo.localdomain

192.168.1.45    wunjo   wunjo.localdomain
#+end_src

Generating the ~kube-vip~ static pods, the VIP is the virtual ip that will be utilized for other nodes to join. More can be read at [[https://kube-vip.io/docs/installation/static/]].

#+begin_src bash
export VIP=192.168.1.40
export INTERFACE="lo"
export KVVERSION="$(curl -sL https://api.github.com/repos/kube-vip/kube-vip/releases | jq -r '.[0].name')"

sudo ctr image pull ghcr.io/kube-vip/kube-vip:$KVVERSION
sudo ctr run --net-host --rm ghcr.io/kube-vip/kube-vip:$KVVERSION vip \
        /kube-vip manifest pod \
        --interface $INTERFACE \
        --address $VIP \
        --controlplane \
        --services \
        --bgp \
        --localAS 65000 \
        --bgpRouterID 192.168.1.2 \
        --bgppeers 192.168.1.102:65000::false,192.168.1.103:65000::false,192.168.1.104:65000::false | sudo tee /etc/kubernetes/manifests/kube-vip.yaml
#+end_src

I will copy the generated manifest to the same filepath (~/etc/kubernetes/manifests/~) on my other two nodes, so when I run the join command on those two nodes it will pick up this static pod manifest and create the vip.

** Running ~kubeadm~

Now pick one of the nodes and we will refer to this node as the main node going forward.

Create a bootstrap kubeadm yaml file, where the ~controlPlaneEndpoint~ is the VIP we defined in the previous step. I will also be disabling ~kube-proxy~ as the CNI I will be installing can be utilized as a replacement.

#+begin_src yaml
# kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
skipPhases:
  - addon/kube-proxy
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
controlPlaneEndpoint: "192.168.1.40:6443"
networking:
  podSubnet: "10.85.0.0/16"
  serviceSubnet: "10.10.10.0/23"
clusterName: "my-awesome-cluster"
---
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
cgroupDriver: systemd
#+end_src

#+begin_quote

#+end_quote

I will run the following command on the main node to setup the first control-plane:

#+begin_src shell
sudo kubeadm init --config ./kubeadm-config.yaml --upload-certs
#+end_src

The above will run the ~kubeadm~ configuration and will output two ~kubeadm~ commands (control plane and worker) to run on my other two nodes, taking note of the specific join command for a ~control plane~.

Before the node can become ready, we must install a CNI so that CoreDNS can go from a ~Pending~ to a ~Running~ status. I will be using ~cilium~ as my CNI and installing it through helm.

** Install Helm

#+begin_src shell
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
#+end_src

** Installing ~cilium~ CNI

Since ~cilium~ will also be replacing ~kube-proxy~ we need to pass along some extra parameters when installing it. We need to set ~kubeProxyReplacement~ as ~true~ and define the ~k8sServiceHost~ and ~k8sServicePort~ which is the virtual IP we defined through ~kube-vip~.

#+begin_src shell
helm repo add cilium https://helm.cilium.io/
helm install \
    cilium \
    cilium/cilium \
    --namespace kube-system \
    --set=kubeProxyReplacement=true \
    --set=k8sServiceHost=192.168.1.40 \
    --set=k8sServicePort=6443
#+end_src

** Joining the Other Control Planes

Once this first control plane becomes ~Ready~, I can continue on with the other two nodes. 

#+begin_src
# check node status
kubectl get nodes -o wide
#+end_src

Since I am going to make my three nodes control planes and workers, I will run the ~kubeadm~ for adding another control plane to my other two nodes. It should look something like this:

#+begin_src shell
sudo kubeadm join 192.168.1.40:6443 --token %token% \
    --discovery-token-ca-cert-hash %hash% \
    --control-plane \
    --certificate-key %cert-key%
#+end_src

Once all three nodes are in the ~Ready~ status, we have to remove the taint for the control plane so we can schedule pods on these nodes as if they were worker nodes.

#+begin_src shell
kubectl taint nodes --all  node-role.kubernetes.io/control-plane-
#+end_src

With this done, I now have my three node cluster ready!
