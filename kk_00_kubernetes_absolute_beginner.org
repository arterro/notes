:PROPERTIES:
:ID:       acbcd7fd-a200-4da9-911a-abadca83f1ca
:END:
#+title: Kubernetes Absolute Beginner
#+hugo_custom_front_matter: :summary Notes pertaining to the KodeKloud Kubernetes Absolute Beginner course
#+hugo_lastmod: Time-stamp: <2022-04-27 22:09:26 (adelgado)>
#+hugo_code_fence: nil
#+hugo_section: notes
#+hugo_draft: false
#+hugo_tags: kubernetes
#+property: header-args :dir run :mkdirp yes :padline no :exports both :results code :eval no-export

* Overview

Kubernetes (K8s) was initially built by Google but is now an open-source project (CNCF).

** Containers

Containers are not a new concept and they've been around for 10+ years. They exist in other forms like LXC (hard, low-level) but Docker has become the defacto standard due to the ease it brings at a higher level to create containers.

Containers are completely isolated environments that can contain their own processes / network interfaces / mounts but share the same OS kernel.

Sharing the same OS kernel means that Docker can run any OS as long as it shares the kernel of the host operating system. An example woul be Ubuntu, a linux kernel, allows us to run any type of Linux OS in a container like Gentoo, Debian, Arch and so forth. Alternatively, we would not be able to run a Windows container on a linux based host as the kernerls differ and said container would need to be ran on a Windows host.

** Container Orchestration

Container Orchestration is the process of automatically deploying and managing containers, as such, Kubernetes is an orchestration technology.

*Advantages*
+ Application is highly available, hardware failures don't bring application down
+ Multiple instances of the application across different nodes
+ User traffic is load balanced across containers
+ Low hardware resources; easily scale up/down

** Kubernetes Architecture

  : Nodes
  : -----
  : A physical / virtual worker machine where K8s is installed and containers are launched

If the node fails, what happens? Application goes down! (╥﹏╥)
Don't fear cause we have clusters!

  : Cluster
  : -----
  : Set of nodes grouped together

If one node fails the application keeps chugging about as it can be accessed from the other nodes. Having multiple nodes also helps in sharing load.

Cool, who's gonna manage all of this (the cluster)?

  : Master
  : -----
  : A node with k8s installed on it, configured as a master that watches over the nodes in the cluster.

Watches over the nodes in the cluster and is responsible for the orchestration of containrs on the worker nodes.

*** Components

When installing Kubernetes the following components are installed:

+ *API Server*
  - acts as the front-end for Kubernetes which users, management devices and CLI's communicate with to interact with clusters.
+ *etcd*
  - is a distributed reliable key-value store where Kubernetes stores all data used to manage the cluster. Responsible for implementing locks within the cluster to ensure there is no conflict between the masters.
+ *kubelet*
  - is the agent that runs on each node in the cluster making sure that the containers are running on the ndoes
+ *Container Runtime*
  - is the underlying software used to run contaiuners.
+ *Controller*
  - are the brains for orchestration, which are responsible for noticing and responding when nodes / containers / endpoints go down.
+ *Scheduler*
  - is responsible for distributing work / containers across multiple nodes by looking for newly created containers.

*** Master vs Worker Nodes

Worker nodes (minions) is where the containers are hosted.

Master nodes have the *kubelet* agent responsible for interacting with a master to provide health information on the worker nodes and carry out actions on them as well.

All information gathered on the master node is stored in a key-value store using *etcd*.

Also containers a *controller* and a *scheduler*.

* Kubernetes Setup

Can be setup locally directly on your host machine or a VM using solutions like *minikube* (dev friendly), *MicroK8s*, *Kubeadm* (production).

Can also be setup on the cloud using *Google Cloud Platform*, *Amazon Web Services*, *Azure*.

minikube is developer friendly and good for testing purposes, as it bundles all the different components into a single image, providing a pre-configured single node Kubernetes cluster provided as an ISO image.

* Kubernetes Concepts

Kubernetes does not deploy containers directly on the worker nodes; instead are encapsulated into a Kubernetes object known as a *Pod*.

  : Pod
  : -----
  : Single instance of an application, the smallest object that can be created in Kubernetes.

Pods share a 1:1 relation with container images running the application. If you need to scale up, you add more pods and remove pods to scale down.

Pods can contain multiple images, doesn't have to be just one. For example, in the case of needing a helper container to go along with the application container.

#+begin_src sh
# Creating a Pod with image pulled from Dockerhub
kubectl run nginx --image nginx
#+end_src

#+RESULTS:
#+begin_src sh
pod/nginx created
#+end_src

#+begin_src sh
# View Pods
kubectl get pods
#+end_src

#+RESULTS:
#+begin_src sh
NAME                     READY   STATUS    RESTARTS      AGE
nginx                    1/1     Running   0             5s
#+end_src

* PODs, ReplicaSets and Deployments

** Pods with YAML

Kubernetes uses YAML files as inputs for the creation of objects and follows the following structure for the four top level fields:

#+begin_src yaml :tangle run/pod-definition.yaml :eval no
# pod-definition.yaml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx
#+end_src

#+begin_src sh
# Command to create object using YAML
kubectl create -f pod-definition.yaml
#+end_src

#+RESULTS:
#+begin_src sh
pod/myapp-pod created
#+end_src

*Example of Kubernetes objects and their versions:*

#+attr_html: :class table-striped table-hover
| Kind       | Version |
|------------+---------|
| Pod        | v1      |
|------------+---------|
| Service    | v1      |
|------------+---------|
| ReplicaSet | apps/v1 |
|------------+---------|
| Deployment | apps/v1 |
|------------+---------|

*Four common top level fields that reside in a manifest file, YAML data type is defined within brackets []*:

  : apiVersion [string]
  : -----
  : Defines the api version of Kubernetes to use to create the object.

  : kind [string]
  : -----
  : Refers to the type of object to be created.

  : metadata [dictionary]
  : -----
  : Definition data of the object like name, labels, etc. To note, you can only specify properties that Kubernetes expects under metadata, for labels it can be a user defined key-value pair.

  : spec [dictionary]
  : -----
  : Defines the specification for the object kind to be created.

** ReplicationControllers and ReplicaSets

Controllers are the brains behind Kubernetes, processes that monitor Kubernetes objects.

ReplicationController / ReplicaSets helps us manage multiple instances of a single pod. Say you have an application in a pod and that goes ლ(ಠ益ಠ)ლ, you'd want to have another instance running so users don't lose access to the application.

Even if using one instance, the ReplicationController / ReplicaSet will ensure that if the single pod goes unhealthy, a new pod instance is brought up. It makes it so our application has high availability, which also helps with load balancing and scaling. If the load increases we can spin up additional instances or even expand to new nodes within the cluster if resources start to become limited.

*** ReplicationController

ReplicationController is an older technology which will be replaced by ReplicaSet.

#+begin_src yaml :tangle run/rc-definition.yaml
# rc-definition.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  replicas: 3
#+end_src

#+begin_src sh
# Command to create the replication controller
kubectl create -f rc-definition.yaml
#+end_src

#+RESULTS:
#+begin_src sh
replicationcontroller/myapp-rc created
#+end_src

#+begin_src sh
# Command to get replication controllers
kubectl get replicationcontroller
#+end_src

#+RESULTS:
#+begin_src sh
NAME       DESIRED   CURRENT   READY   AGE
myapp-rc   3         3         3       9s
#+end_src

#+begin_src sh
# Command to get pods
kubectl get po
#+end_src

#+RESULTS:
#+begin_src sh
NAME                     READY   STATUS    RESTARTS        AGE
myapp-rc-bdrmp           1/1     Running   0               15s
myapp-rc-cxcd5           1/1     Running   0               15s
myapp-rc-xtrnp           1/1     Running   0               15s
#+end_src

*** ReplicaSet

Recommended way to setup replication.

Aside from the ~apiVersion~ and the ~kind~ differing from the YAML construct of the ReplicaController, a property that must be defined is a ~selector~.

A ~selector~ defines what pods fall under the ReplicaSet; you may notice pods are defined in the ~template~ of the ReplicaSet, but a ReplicaSet can manage pods /not/ defined in the YAML configuration, like pods created prior to the ReplicaSet.

#+begin_src yaml :tangle run/replicaset-definition.yaml
# replicaset-definition.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  replicas: 3
  selector:
    matchLabels:
      type: front-end
#+end_src

#+begin_src sh
# Command to create the replica set
kubectl create -f replicaset-definition.yaml
#+end_src

#+RESULTS:
#+begin_src sh
replicaset.apps/myapp-replicaset created
#+end_src

#+begin_src sh
# Command to get replica sets
kubectl get replicaset
#+end_src

#+RESULTS:
#+begin_src sh
NAME               DESIRED   CURRENT   READY   AGE
myapp-replicaset   3         3         3       25s
#+end_src

#+begin_src sh
# Command to get pods
kubectl get po
#+end_src

#+RESULTS:
#+begin_src sh
NAME                     READY   STATUS    RESTARTS   AGE
myapp-replicaset-lxmnh   1/1     Running   0          31s
myapp-replicaset-q8mdz   1/1     Running   0          31s
myapp-replicaset-vsjqr   1/1     Running   0          31s
#+end_src

The ~template~ section is required in case of a pod failure; ReplicaSet needs to create a new one to keep within the replicas defined and to do so the ReplicaSet ~template~ section is required.

*Scaling*

There are three ways to scale a ReplicaSet if necessary:

*FIRST METHOD*\\
Edit the definition file and edit the ~replicas~ property and run the ~kubectl replace~ command to update the ReplicaSet

#+begin_src yaml :tangle run/replicaset-definition.yaml
# replicaset-definition.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: front-end
spec:
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
  replicas: 10
  selector:
    matchLabels:
      type: front-end
#+end_src

#+begin_src sh
kubectl replace -f replicaset-definition.yaml
#+end_src

#+RESULTS:
#+begin_src sh
replicaset.apps/myapp-replicaset replaced
#+end_src

#+begin_src sh
kubectl get replicaset
#+end_src

#+RESULTS:
#+begin_src sh
NAME               DESIRED   CURRENT   READY   AGE
myapp-replicaset   10        10        10      6m18s
#+end_src

*SECOND METHOD*\\
Running the ~kubectl scale~ command and referencing how many replicas are wanted and to which definition file / ReplicaSet name to apply

#+begin_src sh
# Definition File
# Does not modify the original value in the file
kubectl scale --replicas 6 -f replicaset-definition.yaml
#+end_src

#+RESULTS:
#+begin_src sh
replicaset.apps/myapp-replicaset scaled
#+end_src

#+begin_src sh
kubectl get replicaset
#+end_src

#+RESULTS:
#+begin_src sh
NAME               DESIRED   CURRENT   READY   AGE
myapp-replicaset   6         6         6       6m28s
#+end_src

#+begin_src sh
# ReplicaSet Name
kubectl scale --replicas 2 replicaset myapp-replicaset
#+end_src

#+RESULTS:
#+begin_src sh
replicaset.apps/myapp-replicaset scaled
#+end_src

#+begin_src sh
kubectl get replicaset
#+end_src

#+RESULTS:
#+begin_src sh
NAME               DESIRED   CURRENT   READY   AGE
myapp-replicaset   2         2         2       8m23s
#+end_src

*THIRD METHOD*\\
Can be setup by load -- but too advanced for this primer ༼☯﹏☯༽

*Labels and Selectors*

It is the key to help in managing the many different resources that are available within Kubernetes. In this instance, we may have hundreds of pods deployed and if we wish to monitor them via a ReplicationController / ReplicaSet we can do so via the labels we define on the pods at creation.

* Deployments

#+begin_src yaml :tangle run/deployment-definition.yaml
# deployment-definition.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  replicas: 3
  selector:
    matchLabels:
      type: front-end
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx
#+end_src

#+begin_src sh
# Command to create the deployment from the above manifest file
kubectl create -f deployment-definition.yaml
#+end_src

#+RESULTS:
#+begin_src sh
deployment.apps/myapp-deployment created
#+end_src

#+begin_src sh
# Command to get all deployments
kubectl get deployments
#+end_src

#+RESULTS:
#+begin_src sh
NAME               READY   UP-TO-DATE   AVAILABLE   AGE
myapp-deployment   3/3     3            3           4s
#+end_src
